{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99411a8c-7f6f-4916-9c12-a7860e258abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EGMS DATA DOWNLOADER for BUILDING ANOMALY DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8509706-1be1-4f71-a056-e080940d81fd",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8328ea-6bcc-43ae-9817-3a5e5487ca30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import osmnx as ox\n",
    "import json\n",
    "import zipfile\n",
    "import uuid\n",
    "import requests\n",
    "import shutil\n",
    "import glob\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from io import BytesIO\n",
    "from joblib import Parallel, delayed\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from shapely.ops import unary_union\n",
    "from shapely.wkt import dumps, loads\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from pyrosm import get_data, OSM\n",
    "import concurrent.futures\n",
    "\n",
    "# Configure inline plotting for Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress specific warnings from libraries\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b95b907-becd-425b-a809-cada240af1de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON has been saved to data/insar_data_download_updated2.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming your JSON data is in a file named 'data.json'\n",
    "input_file_path = 'data/insar_data_download_updated.json'\n",
    "output_file_path = 'data/insar_data_download_updated2.json'\n",
    "\n",
    "# Load the JSON data from a file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to add a bbox attribute and an empty intersection to each zip file URL in the JSON data\n",
    "def add_bbox_and_intersection_to_urls(data):\n",
    "    for country, cities in data.items():\n",
    "        for city, directions in cities.items():\n",
    "            for direction, urls in directions.items():\n",
    "                new_url_entries = []\n",
    "                for url in urls:\n",
    "                    new_url_entries.append({\n",
    "                        \"url\": url,\n",
    "                        \"bbox\": []  # Initialize bbox as empty list\n",
    "                    })\n",
    "                directions[direction] = new_url_entries\n",
    "            # Initialize an empty Intersection key at the same level as Ascending/Descending\n",
    "            directions['Intersection'] = \"\"  # Initialize intersection as an empty string\n",
    "\n",
    "# Modify the data\n",
    "add_bbox_and_intersection_to_urls(data)\n",
    "\n",
    "# Save the modified data back to a new JSON file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Updated JSON has been saved to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86424a6e-b154-4a2e-8e8e-a435966074ec",
   "metadata": {},
   "source": [
    "## Download and Extract InSAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a743585-22e4-4022-bde3-c1bb59f07e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_extract_insar_data(base_dir, json_config_path, download_id):\n",
    "    \n",
    "    def _download_file(url, filename, extract_dir, base_name):\n",
    "        \n",
    "        def _is_valid_download(extract_dir, base_name):\n",
    "            subfolder_path = os.path.join(extract_dir, base_name)\n",
    "            if os.path.exists(subfolder_path) and os.path.isdir(subfolder_path):\n",
    "                xml_file = os.path.join(subfolder_path, base_name + \".xml\")\n",
    "                csv_file = os.path.join(subfolder_path, base_name + \".csv\")\n",
    "                return os.path.exists(xml_file) and os.path.exists(csv_file) and \\\n",
    "                       os.path.getsize(xml_file) > 0 and os.path.getsize(csv_file) > 0\n",
    "            return False\n",
    "        \n",
    "        if _is_valid_download(extract_dir, base_name):\n",
    "            progress.set_postfix_str(f\"Files for {base_name} already downloaded and valid. Skipping...\")\n",
    "            return \"Skipped\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            with open(filename, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            if os.path.getsize(filename) < 1024:\n",
    "                with open(filename, 'r') as file:\n",
    "                    content = file.read()\n",
    "                if \"valid download token\" in content:\n",
    "                    os.remove(filename)\n",
    "                    return False\n",
    "            return True\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            tqdm.write(f\"Error downloading {url}: {e}\")\n",
    "            if os.path.exists(filename):\n",
    "                os.remove(filename)\n",
    "            return False\n",
    "    \n",
    "    def _extract_to_subfolder(zip_file_name, extract_dir):\n",
    "        if not os.path.exists(zip_file_name):\n",
    "            tqdm.write(f\"No zip file to extract: {zip_file_name}\")\n",
    "            return False\n",
    "        base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
    "        subfolder_path = os.path.join(extract_dir, base_name)\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(subfolder_path)\n",
    "        return True\n",
    "    \n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    #total_urls = sum(len(urls) for _, cities in json_data.items() for _, directions in cities.items() for _, urls in directions.items())\n",
    "    #total_urls = sum(len(direction['urls']) for _, cities in json_config.items() for _, directions in cities.items() for direction in directions.values())\n",
    "    total_urls = sum(len(urls) for cities in json_data.values() for directions in cities.values() for key, urls in directions.items() if key != 'Intersection')\n",
    "\n",
    "    progress = tqdm(total=total_urls, desc='Overall Progress')\n",
    "\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for country, cities in json_data.items():\n",
    "        country_dir = os.path.join(base_dir, country)\n",
    "        os.makedirs(country_dir, exist_ok=True)\n",
    "        for city, directions in cities.items():\n",
    "            city_dir = os.path.join(country_dir, city)\n",
    "            os.makedirs(city_dir, exist_ok=True)\n",
    "            for direction, urls in directions.items():\n",
    "                if 'Intersection' in direction: continue\n",
    "                direction_dir = os.path.join(city_dir, direction)\n",
    "                os.makedirs(direction_dir, exist_ok=True)\n",
    "                for url_entry in urls:\n",
    "                    url = url_entry['url']  # Access the URL from the new JSON structure\n",
    "                    url_with_id = f\"{url}?id={download_id}\"\n",
    "                    zip_file_name = os.path.join(direction_dir, os.path.basename(url).split('?')[0])\n",
    "                    base_name = os.path.splitext(os.path.basename(zip_file_name))[0]\n",
    "                    download_result = _download_file(url_with_id, zip_file_name, direction_dir, base_name)\n",
    "                    if download_result == True:\n",
    "                        if not _extract_to_subfolder(zip_file_name, direction_dir):\n",
    "                            os.remove(zip_file_name)\n",
    "                    progress.update(1)\n",
    "\n",
    "    progress.close()\n",
    "    tqdm.write(\"Download and extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714ff0d4-edd2-4859-8e7b-46085f0f11a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 72/72 [00:00<00:00, 1221.23it/s, Files for EGMS_L2a_007_0850_IW1_VV already downloaded and valid. Skipping...]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_root = \"data/insar_downloads\"  \n",
    "json_file_path = 'data/insar_data_download_updated.json'\n",
    "    \n",
    "download_id = \"e3b17ad10e024423b43c509624a05463\"\n",
    "download_and_extract_insar_data(data_root, json_file_path, download_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a75c59-5f12-450f-b1e8-4266024bed75",
   "metadata": {},
   "source": [
    "## Convert Downloaded InSAR CSV Files into GeoPackage Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c01dc286-c0ff-4e3e-9109-eb388a6de4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_csv_to_geopackage(data_root, json_config_path):\n",
    "    # Load JSON configuration\n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_config = json.load(file)\n",
    "        #print(json_config)\n",
    "\n",
    "    total_iterations = sum(len(urls) for _, cities in json_config.items() for _, directions in cities.items() for urls in directions.values())\n",
    "\n",
    "    progress = tqdm(total=total_iterations, desc='Overall Progress')\n",
    "\n",
    "    for country, cities in json_config.items():\n",
    "        for city, directions in cities.items():\n",
    "            for direction, url_entries in directions.items():\n",
    "                for url_entry in url_entries:\n",
    "                    url = url_entry['url']\n",
    "                    bbox = url_entry['bbox']  # This will store the bounding box\n",
    "                    folder_name = os.path.basename(url).split('.')[0]\n",
    "                    folder_path = os.path.join(data_root, country, city, direction, folder_name)\n",
    "\n",
    "                    if not bbox:  # If bbox is empty, processing is needed\n",
    "                        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "                            for filename in os.listdir(folder_path):\n",
    "                                if filename.endswith(\".csv\"):\n",
    "                                    file_without_extension = os.path.splitext(filename)[0]\n",
    "                                    file_path_without_extension = os.path.join(folder_path, file_without_extension)\n",
    "                                    try:\n",
    "                                        gdf = read_csv_or_geopackage_to_geopandas(file_path_without_extension)\n",
    "                                        if not gdf.empty:\n",
    "                                            # Successfully converted and not previously saved bbox\n",
    "                                            bbox = gdf.total_bounds  # Get the bounding box of the GeoDataFrame\n",
    "                                            url_entry['bbox'] = bbox.tolist()  # Update the bbox in json data\n",
    "                                            # Save the updated JSON configuration back to the file\n",
    "                                            with open(json_config_path, 'w') as f:\n",
    "                                                json.dump(json_config, f, indent=4)\n",
    "                                        progress.set_postfix_str(f\"Converted {filename} in {city}/{direction}\")\n",
    "                                    except FileNotFoundError as e:\n",
    "                                        progress.set_postfix_str(str(e))\n",
    "                        # Update progress after each URL is processed\n",
    "                    else:\n",
    "                        progress.set_postfix_str(f\"Skipped {folder_name} in {city}/{direction}\")\n",
    "                    progress.update(1)\n",
    "\n",
    "    progress.set_postfix_str(f\"Complete\")\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def read_csv_or_geopackage_to_geopandas(file_name,verbose=False):\n",
    "    \"\"\"\n",
    "    Reads a GeoJSON or CSV file and converts it to a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): The name of the file without the file extension.\n",
    "\n",
    "    Returns:\n",
    "        gdf (GeoDataFrame): The GeoDataFrame containing the data from the file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If no GeoJSON or CSV file is found with the given file name.\n",
    "    \"\"\"\n",
    "    def _displacement2float32(displacement_str):\n",
    "        \"\"\"Converts  displacements to float32 precision.\"\"\"\n",
    "        # Convert the string to a dictionary\n",
    "        displacement_dict = json.loads(displacement_str)\n",
    "        # Convert each value in the dictionary to float32\n",
    "        return {date: round(value, 3) for date, value in displacement_dict.items()}\n",
    "\n",
    "    geo_path = f\"{file_name}.geojson\"\n",
    "    csv_path = f\"{file_name}.csv\"\n",
    "\n",
    "    # TODO: activate it later, it was deactivated to overwrite everything once.\n",
    "    # Check if GeoJSON file exists\n",
    "    #if os.path.exists(geo_path):\n",
    "    #    if verbose: print(\"File is loading: {} ...\".format(geo_path))\n",
    "    #    gdf = gpd.read_file(geo_path, engine=\"pyogrio\")\n",
    "    # Check if CSV file exists\n",
    "    if os.path.exists(csv_path):\n",
    "        if verbose: print(\"File is loading: {} ...\".format(csv_path))\n",
    "        df = pd.read_csv(csv_path,engine='c')\n",
    "\n",
    "        # Define the target columns for the GeoDataFrame\n",
    "        target_cols = ['latitude', 'longitude']\n",
    "        displacements_cols = df.columns[df.columns.str.isdigit()].tolist()\n",
    "\n",
    "        # Create a new column 'displacements' containing dictionaries of non-null values from date columns\n",
    "        df['displacements'] = df[displacements_cols].apply(lambda x: x.dropna().to_dict(), axis=1)\n",
    "        \n",
    "        # Drop the individual date columns\n",
    "        df.drop(columns=displacements_cols, inplace=True)\n",
    "        \n",
    "        df['displacements'] = df['displacements'].apply(_displacement2float32)\n",
    "\n",
    "        # Create a GeoDataFrame from the DataFrame\n",
    "        geometry = gpd.points_from_xy(df['longitude'], df['latitude'])\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "        # Convert the GeoDataFrame to a GeoJSON file\n",
    "        gdf.to_file(geo_path, driver='GeoJSON')\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No GeoJSON or CSV file found for '{file_name}'\")\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a77215-b4b9-42be-9b80-8d96f82de52a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = \"data/insar_downloads\"  \n",
    "json_file_path = 'data/insar_data_download_updated.json'\n",
    "    \n",
    "convert_csv_to_geopackage(data_root, json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934654cc-a644-4ce9-a2bb-15bfb3185730",
   "metadata": {},
   "source": [
    "## Download Eubucco Building Footprints for Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59474216-da1f-451c-b9fe-716c39bd27a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_extract_eubucco_for_countries(data_root, json_config_path, osm_type_path):\n",
    "    def _find_gpkg_in_directory(directory):\n",
    "        \"\"\"Helper function to find the first .gpkg file in a specified directory.\"\"\"\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".gpkg\"):\n",
    "                return os.path.join(directory, file)\n",
    "        return None  # Return None if no .gpkg file is found\n",
    "\n",
    "    # Load the JSON configuration\n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_config = json.load(file)\n",
    "\n",
    "    # First, get the list of countries from the EUBUCCO API\n",
    "    eubucco_api_url = \"https://api.eubucco.com/v0.1/countries\"\n",
    "    response = requests.get(eubucco_api_url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve country list from EUBUCCO\")\n",
    "        return\n",
    "\n",
    "    available_countries = response.json()\n",
    "    available_country_names = {country['name'].lower(): country for country in available_countries}\n",
    "\n",
    "    # Create the root directory if it doesn't exist\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=len(json_config), desc=\"Downloading and extracting data\", unit=\"country\")\n",
    "    \n",
    "    # Loop through the countries specified in the JSON config\n",
    "    for country, cities in json_config.items():\n",
    "        extract_dir = os.path.join(data_root, country)\n",
    "        gpkg_path = _find_gpkg_in_directory(extract_dir)\n",
    "        if gpkg_path and os.path.exists(gpkg_path):\n",
    "            pbar.set_postfix_str(f\"{country} GPKG already exists, skipping download.\")\n",
    "        else:\n",
    "            country_lower = country.lower()\n",
    "            if country_lower in available_country_names:\n",
    "                # Get the download link for the country's GPKG file\n",
    "                gpkg_url = available_country_names[country_lower]['gpkg']['download_link']\n",
    "                zip_file_path = os.path.join(extract_dir, 'eubucco_country.zip')\n",
    "\n",
    "                # Create directory for the country\n",
    "                os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "                # Download the GPKG zip file\n",
    "                zip_response = requests.get(gpkg_url, stream=True)\n",
    "                if zip_response.status_code == 200:\n",
    "                    downloaded = 0\n",
    "                    with open(zip_file_path, 'wb') as f:\n",
    "                        for chunk in zip_response.iter_content(chunk_size=1024*1024):  # Larger chunk size\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            pbar.set_postfix_str(f\"Downloading {country}: {downloaded / 1024**2:.2f} MB\")\n",
    "\n",
    "                    # Extract the GPKG file from the ZIP\n",
    "                    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(extract_dir)\n",
    "                        pbar.set_postfix_str(f\"Extracting {country}\")\n",
    "\n",
    "                    # Optionally remove the ZIP file after extraction\n",
    "                    os.remove(zip_file_path)\n",
    "                    pbar.set_postfix_str(f\"Completed {country}\")\n",
    "                else:\n",
    "                    pbar.set_postfix_str(f\"Failed to download GPKG for {country}\")\n",
    "            else:\n",
    "                pbar.set_postfix_str(f\"{country} not available in EUBUCCO. OSM data is to be downloaded ...\")\n",
    "                fp = get_data(country,directory=extract_dir)\n",
    "                osm = OSM(fp)\n",
    "                buildings = osm.get_buildings()\n",
    "                \n",
    "                type_mapping = pd.read_csv(osm_type_path)\n",
    "                # Map 'type_source' to 'type_db' using a dictionary for faster lookup\n",
    "                type_dict = pd.Series(type_mapping['type_db'].values, index=type_mapping['type_source']).to_dict()\n",
    "    \n",
    "                buildings=clean_buildings_osm(buildings, type_dict)\n",
    "                buildings=buildings.to_crs(epsg=3035)\n",
    "                buildings.to_file(os.path.join(extract_dir,f'{country}.gpkg'),driver='GPKG')\n",
    "                \n",
    "\n",
    "        # Update tqdm progress bar after finishing or skipping a country\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.set_postfix_str(\"Complete\")\n",
    "    pbar.close()\n",
    "\n",
    "    \n",
    "\n",
    "def clean_buildings_osm(osm_gdf, type_dict):\n",
    "    def _replace_nan_with_uuid(x):\n",
    "        if pd.isna(x):\n",
    "            return uuid.uuid4().hex\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def _add_floor_as_height(df, floor_height):\n",
    "        return [x if str(x) != 'nan' else y * floor_height for x, y in zip(df.height, df.floors)]\n",
    "\n",
    "        \n",
    "    # List of columns to clean\n",
    "    columns_to_clean = ['height', 'building:levels', 'start_date']\n",
    "    \n",
    "    # Cleaning each specified column if it exists\n",
    "    for col in columns_to_clean:\n",
    "        if col in osm_gdf.columns:\n",
    "            osm_gdf[col] = osm_gdf[col].replace({None: np.nan, '': np.nan})\n",
    "            osm_gdf[col] = osm_gdf[col].astype(str).str.extract(\"(\\\\d+\\\\.?(\\\\d+)?)\")[0].astype(float)  \n",
    "\n",
    "    # Check if type info is present and replace 'yes' with None\n",
    "    osm_gdf.replace({'yes': None}, inplace=True)\n",
    "\n",
    "    # Determine 'type_source' based on available columns\n",
    "    def get_type_source(row):\n",
    "        for col in ['building', 'building:use', 'amenity']:\n",
    "            if col in row.index and pd.notna(row[col]) and row[col] != '':\n",
    "                return row[col]\n",
    "        return None\n",
    "\n",
    "    # Apply the function across rows to determine 'type_source'\n",
    "    osm_gdf['type_source'] = osm_gdf.apply(get_type_source, axis=1)\n",
    "    \n",
    "    # Map 'type_source' to 'type'\n",
    "    osm_gdf['type'] = osm_gdf['type_source'].map(type_dict).fillna('unknown')\n",
    "    \n",
    "    # Rename columns if they exist\n",
    "    if 'osmid' in osm_gdf.columns:\n",
    "        osm_gdf.rename(columns={'osmid': 'id'}, inplace=True)\n",
    "    if 'start_date' in osm_gdf.columns:\n",
    "        osm_gdf.rename(columns={'start_date': 'age'}, inplace=True)\n",
    "    if 'building:levels' in osm_gdf.columns:\n",
    "        osm_gdf.rename(columns={'building:levels': 'floors'}, inplace=True)\n",
    "        \n",
    "    # Add heights using floor count if applicable\n",
    "    if 'floors' in osm_gdf.columns and 'height' in osm_gdf.columns:\n",
    "        osm_gdf['height'] = _add_floor_as_height(osm_gdf, floor_height=2.5)  # assuming floor height is 2.5 meters\n",
    "\n",
    "        \n",
    "        \n",
    "    # Define the target columns to include in the filtered GeoDataFrame\n",
    "    target_cols = ['id','height','age','type','type_source','geometry']\n",
    "    osm_gdf = osm_gdf.loc[:,target_cols]\n",
    "    \n",
    "    osm_gdf = osm_gdf[osm_gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])]\n",
    "    osm_gdf = osm_gdf[~osm_gdf[\"geometry\"].geom_type.isin([\"LineString\", \"Point\"])]\n",
    "    \n",
    "    osm_gdf = osm_gdf.drop_duplicates().reset_index(drop=True)\n",
    "    osm_gdf['id'] = osm_gdf['id'].apply(_replace_nan_with_uuid)\n",
    "    \n",
    "\n",
    "    return osm_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcb25e2-de8b-41aa-a16b-24293d372297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and extracting data:  56%|█████▌    | 5/9 [00:00<00:00, 1337.47country/s, Great Britain not available in EUBUCCO. OSM data is to be downloaded ...]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Protobuf data 'great-britain-latest.osm.pbf' (1667.83 MB) to:\n",
      "'/p/scratch/hai_eo_hype/kuzu/building_anomaly/building_anomaly_detection/data/insar_downloads/Great Britain/great-britain-latest.osm.pbf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and extracting data: 100%|██████████| 9/9 [1:11:31<00:00, 476.87s/country, Complete]                                                                \n"
     ]
    }
   ],
   "source": [
    "data_root = \"data/insar_downloads\"  \n",
    "json_file_path = 'data/insar_data_download_updated.json'\n",
    "osm_type_path = 'data/osm_type_matches.csv'\n",
    "\n",
    "download_and_extract_eubucco_for_countries(data_root, json_file_path,osm_type_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b448ff-c516-43ee-b846-ee9f24dfad25",
   "metadata": {},
   "source": [
    "## Prepare District Building Footprint Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "166de59e-ea7c-46f7-b39b-54335d27cc9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_building_footprints_for_selected_ROIs(data_root, json_config_path):\n",
    "    \n",
    "    def _calculate_intersection(ascending, descending):\n",
    "        union_ascending = unary_union([box(*url['bbox']) for url in ascending if 'bbox' in url])\n",
    "        union_descending = unary_union([box(*url['bbox']) for url in descending if 'bbox' in url])\n",
    "        intersection=union_ascending.intersection(union_descending)\n",
    "        return intersection\n",
    "    \n",
    "    def _find_gpkg_in_directory(directory):\n",
    "        \"\"\"Helper function to find the first .gpkg file in a specified directory.\"\"\"\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".gpkg\"):\n",
    "                return os.path.join(directory, file)\n",
    "        return None  # Return None if no .gpkg file is found\n",
    "\n",
    "    # Load the JSON configuration\n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_config = json.load(file)\n",
    "    \n",
    "    # Determine the total number of cities to process for setting up the tqdm progress bar\n",
    "    total_cities = sum(len(cities) for _, cities in json_config.items())\n",
    "\n",
    "    # Initialize tqdm with the total number of cities\n",
    "    pbar = tqdm(total=total_cities, desc=\"Processing Cities\")\n",
    "    \n",
    "    # Iterate through each country and its cities in the JSON configuration\n",
    "    for country, cities in json_config.items():\n",
    "        \n",
    "        for city, attributes in cities.items():\n",
    "            # Update tqdm description for current city\n",
    "            pbar.set_description(f\"Processing {city} in {country}\")\n",
    "            \n",
    "            if 'Intersection' in attributes and attributes['Intersection']:  # Skip if intersection is already calculated\n",
    "                 pbar.set_postfix_str(f\"Skipped footprints for {city} in {country}. (Already extracted!)\")\n",
    "                \n",
    "            # Calculate intersection if not already calculated\n",
    "            elif 'Intersection' not in attributes or not attributes['Intersection']:\n",
    "                intersection_area = _calculate_intersection(attributes['Ascending'], attributes['Descending'])\n",
    "                mask_polygon = gpd.GeoSeries([intersection_area], crs='EPSG:4326')\n",
    "\n",
    "                # Construct the full path to the GeoPackage file\n",
    "                gpkg_file=_find_gpkg_in_directory(os.path.join(data_root, country))\n",
    "\n",
    "                # Save the filtered GeoDataFrame under each city folder as building_footprint.gpkg\n",
    "                output_folder = os.path.join(data_root, country, city)\n",
    "                if not os.path.exists(output_folder):\n",
    "                    os.makedirs(output_folder)\n",
    "                output_path = os.path.join(output_folder, 'footprint_buildings.gpkg')\n",
    "                    \n",
    "                # Check if the GeoPackage file is available\n",
    "                if gpkg_file and os.path.exists(gpkg_file):\n",
    "                    # Load the GeoPackage file with a mask\n",
    "                    pbar.set_postfix_str(f\"Reading footprints for {city} in {country}.\")\n",
    "                    \n",
    "                    geometry = mask_polygon.to_crs(epsg=3035).iloc[0]\n",
    "                    df = gpd.read_file(gpkg_file, mask=geometry).to_crs(epsg=4326)\n",
    "\n",
    "                    df.to_file(output_path, driver=\"GPKG\")\n",
    "                    \n",
    "                    attributes['Intersection'] = dumps(intersection_area)  # Save as WKT\n",
    "                    with open(json_config_path, 'w') as f:\n",
    "                        json.dump(json_config, f, indent=4)\n",
    "                        pbar.set_postfix_str(f\"Processed and saved footprints for {city} in {country}.\")\n",
    "                    \n",
    "                else:\n",
    "                    # TODO: Implement additional error handling or alternative actions here\n",
    "                    tqdm.write(f'No building footprint info available for {city}/{country}')\n",
    "                          \n",
    "\n",
    "            # Update the progress bar after processing each city\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Close the tqdm progress bar after all processing is complete\n",
    "    pbar.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20115511-2162-4a33-81ee-fe7daf93f14c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Athens in Greece: 100%|██████████| 11/11 [00:00<00:00, 1394.97it/s, Skipped footprints for Athens in Greece. (Already extracted!)]               \n"
     ]
    }
   ],
   "source": [
    "data_root = \"data/insar_downloads\"  \n",
    "json_file_path = 'data/insar_data_download_updated.json'\n",
    "prepare_building_footprints_for_selected_ROIs(data_root, json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a58e7-c6a0-46a5-bbd1-6000143326e1",
   "metadata": {},
   "source": [
    "## Find Points within Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e037fbd-5f8e-40f8-ba04-7ef3e0698c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_points_within_buildings(json_config_path, data_root):\n",
    "    \n",
    "    def _process_points_within_building(building, points, folder_name, city_dir, building_id, origin, pbar=None):\n",
    "        \n",
    "        def _displacement2float32(displacement_str): #TODO this is temporary, later remove it\n",
    "            \"\"\"Converts  displacements to float32 precision.\"\"\"\n",
    "            # Convert the string to a dictionary\n",
    "            displacement_dict = json.loads(displacement_str)\n",
    "            # Convert each value in the dictionary to float32\n",
    "            return {date: round(value, 3) for date, value in displacement_dict.items()}\n",
    "\n",
    "        #points=geo_state_points[geo_state_points.geometry.within(building.geometry)]\n",
    "        if not points.empty:\n",
    "            \n",
    "            output_dir = os.path.join(city_dir, f'buildings_{folder_name.lower()}')\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_file = os.path.join(output_dir, f\"{building_id}_points.geojson\")\n",
    "\n",
    "            # Add additional information to the points\n",
    "            points = points.copy()\n",
    "            points['b_id'] = building.id\n",
    "            points['b_height'] = building.height\n",
    "            points['b_age'] = building.age\n",
    "            points['b_type'] = building.type\n",
    "            points['b_type_source'] = building.type_source\n",
    "            points['origin'] = origin  # add the origin attribute\n",
    "            points['displacements'] = points['displacements'].apply(_displacement2float32)\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(output_file):\n",
    "                # Read the existing GeoJSON file\n",
    "                existing_points = gpd.read_file(output_file)\n",
    "                # Append new points to the existing points\n",
    "                combined_points = gpd.GeoDataFrame(pd.concat([existing_points, points], ignore_index=True), crs=existing_points.crs)\n",
    "                # Remove duplicates based on the geometry column\n",
    "                combined_points = combined_points.drop_duplicates(subset=['geometry'])\n",
    "            else:\n",
    "                combined_points = points\n",
    "\n",
    "            # Save or overwrite the GeoJSON file with updated points\n",
    "            combined_points.to_file(output_file, driver='GeoJSON')\n",
    "            if pbar:\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(file=f\"Saved {building_id}\", refresh=True)\n",
    "            \n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_config = json.load(file)\n",
    "\n",
    "    for country, cities in json_config.items():\n",
    "        for city, attributes in cities.items():\n",
    "            city_dir = os.path.join(data_root, country, city)\n",
    "            building_file = os.path.join(city_dir, 'footprint_buildings.gpkg')\n",
    "            if not os.path.exists(building_file):\n",
    "                print(f\"No building footprint file found for {city} in {country}\")\n",
    "                continue\n",
    "\n",
    "            buildings = gpd.read_file(building_file, engine=\"pyogrio\")\n",
    "            intersection_area = loads(attributes['Intersection'])\n",
    "            \n",
    "            # Create a map from building ID to index for quick lookup\n",
    "            building_index_map = {building.id: idx for idx, building in enumerate(buildings.itertuples())}\n",
    "\n",
    "\n",
    "            for direction in ['Ascending', 'Descending']:\n",
    "                file_list = attributes.get(direction, [])\n",
    "                for file_info in file_list:\n",
    "                    \n",
    "                    url = file_info['url']\n",
    "                    origin = os.path.basename(url).split('.')[0]\n",
    "                    folder_path = os.path.join(city_dir, direction, origin)\n",
    "                    points_file = os.path.join(folder_path, f\"{origin}.geojson\")\n",
    "                    \n",
    "                    processed = file_info.get('processed')\n",
    "                    if processed == 'complete':\n",
    "                        tqdm.write(f\"Skipping {origin} points in buildings in {city}/{direction} (Already processed!)\")\n",
    "                        continue  # Skip already processed files\n",
    "\n",
    "                    tqdm.write(f\"Processing file {origin} for {city}/{direction}...\")\n",
    "                    geo_state_points = gpd.read_file(points_file, engine=\"pyogrio\", bbox=box(*intersection_area.bounds))\n",
    "\n",
    "                    starting_index = building_index_map.get(processed, 0)\n",
    "                    with tqdm(buildings.itertuples(), total=len(buildings), desc=f\"Processing {origin} points in buildings in {city}/{direction}\") as pbar:\n",
    "                        \n",
    "                        #with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                        #    futures = [executor.submit(_process_points_within_building, building, geo_state_points, direction, city_dir, building.id, origin, pbar) \n",
    "                        #               for building in buildings.itertuples() if building.Index >= starting_index]\n",
    "\n",
    "                        \n",
    "                        for building in pbar:\n",
    "                            if pbar.n < starting_index:\n",
    "                                continue  # Skip buildings until reaching the starting index\n",
    "                            points_within_building = geo_state_points[geo_state_points.geometry.within(building.geometry)]\n",
    "                            _process_points_within_building(building, points_within_building, direction, city_dir, building.id, origin)\n",
    "                            file_info['processed'] = building.id\n",
    "                            with open(json_config_path, 'w') as file:\n",
    "                                json.dump(json_config, file, indent=4)\n",
    "\n",
    "\n",
    "                    # Update the file info to mark as processed\n",
    "                    file_info['processed'] = 'complete'\n",
    "                    geo_state_points=None\n",
    "                    # Write the updated JSON configuration back to the file\n",
    "                    with open(json_config_path, 'w') as file:\n",
    "                        json.dump(json_config, file, indent=4)\n",
    "            \n",
    "            buildings=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ab1a6-0c29-47df-b80a-8d1c19a23895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping EGMS_L2a_146_0238_IW1_VV points in buildings in Pertusillo/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_124_0835_IW1_VV points in buildings in Pertusillo/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_124_0835_IW2_VV points in buildings in Pertusillo/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_124_0836_IW1_VV points in buildings in Pertusillo/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_117_0246_IW2_VV points in buildings in Rome/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_117_0247_IW2_VV points in buildings in Rome/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_095_0825_IW1_VV points in buildings in Rome/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_095_0826_IW1_VV points in buildings in Rome/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_044_0284_IW1_VV points in buildings in Munich/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_044_0285_IW1_VV points in buildings in Munich/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_044_0286_IW1_VV points in buildings in Munich/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0787_IW1_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0788_IW1_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0789_IW1_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0787_IW2_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0788_IW2_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_168_0789_IW2_VV points in buildings in Munich/Descending (Already processed!)\n",
      "Skipping EGMS_L2a_146_0310_IW1_VV points in buildings in Berlin/Ascending (Already processed!)\n",
      "Skipping EGMS_L2a_146_0310_IW2_VV points in buildings in Berlin/Ascending (Already processed!)\n",
      "Processing file EGMS_L2a_146_0311_IW1_VV for Berlin/Ascending...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EGMS_L2a_146_0311_IW1_VV points in buildings in Berlin/Ascending: 100%|█████████▉| 1362044/1362842 [28:52<00:31, 25.07it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file EGMS_L2a_146_0311_IW2_VV for Berlin/Ascending...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:   1%|          | 16623/1362842 [1:09:11<90:38:11,  4.13it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:   5%|▍         | 63444/1362842 [4:19:58<87:05:32,  4.14it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:   8%|▊         | 102241/1362842 [6:58:13<85:19:00,  4.10it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:   9%|▊         | 119051/1362842 [8:07:19<84:21:21,  4.10it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  10%|▉         | 130498/1362842 [8:54:48<83:37:56,  4.09it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  10%|█         | 138504/1362842 [9:27:57<83:31:33,  4.07it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  12%|█▏        | 160807/1362842 [10:59:19<84:02:57,  3.97it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  13%|█▎        | 176654/1362842 [12:04:13<81:06:09,  4.06it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  18%|█▊        | 251935/1362842 [17:15:41<75:16:26,  4.10it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing EGMS_L2a_146_0311_IW2_VV points in buildings in Berlin/Ascending:  19%|█▉        | 262721/1362842 [18:00:23<74:15:32,  4.12it/s]"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "data_root = \"data/insar_downloads\"  \n",
    "json_file_path = 'data/insar_data_download_updated.json'\n",
    "find_points_within_buildings(json_file_path, data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6dc4-f87d-4424-8831-ea7d33b83957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b6cf4-d330-4585-8dfc-3b133eeb859f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1886587c-0188-4608-9687-6c5f4554cd9d",
   "metadata": {},
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6764fb4-8607-4197-a06d-948a68fd0a56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting building footprints for the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "261a6462-3142-4c11-8594-f265893491bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_buildings_by_geometry(base_dir, geometry, osm_type_path):\n",
    "    def _replace_nan_with_uuid(x):\n",
    "        if pd.isna(x):\n",
    "            return uuid.uuid4().hex\n",
    "        else:\n",
    "            return x\n",
    "    \"\"\"\n",
    "    Retrieves and filters building geometries based on specified geometry using a tiling approach,\n",
    "    applies a buffer, and writes the filtered data to a GeoJSON file.\n",
    "\n",
    "    Args:\n",
    "        geometry (Polygon): The polygon geometry within which to filter buildings.\n",
    "        file_name (str): Name of the GeoJSON file to save the filtered buildings.\n",
    "        basedir (str): Directory path where the data files are stored.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Filtered building geometries as a GeoDataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    type_mapping = pd.read_csv(osm_type_path)\n",
    "    # Map 'type_source' to 'type_db' using a dictionary for faster lookup\n",
    "    type_dict = pd.Series(type_mapping['type_db'].values, index=type_mapping['type_source']).to_dict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the bounding box of the geometry\n",
    "    minx, miny, maxx, maxy = geometry.bounds\n",
    "        \n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "\n",
    "        # Calculate the desired width and height of each rectangle\n",
    "    desired_width = 0.125\n",
    "    desired_height = 0.125\n",
    "\n",
    "        # Calculate the number of rows and columns needed to split the bbox into rectangles\n",
    "    num_rows = int(height / desired_height)\n",
    "    num_cols = int(width / desired_width)\n",
    "\n",
    "        # Calculate the actual width and height of each rectangle based on the number of rows and columns\n",
    "    actual_width = width / num_cols\n",
    "    actual_height = height / num_rows\n",
    "        \n",
    "\n",
    "        # Create a grid of bounding boxes within the geometry\n",
    "    all_buildings = []\n",
    "        \n",
    "        # Create the temporary directory path\n",
    "    temp_dir = os.path.join(base_dir, \"temp\")\n",
    "\n",
    "        # Create the temporary directory if it doesn't exist\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    #print(\"Temporary Directory Created: {}\".format(temp_dir))\n",
    "        \n",
    "    rectangles = []\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "                # Calculate the bounds of the smaller bbox\n",
    "            rect_minx = minx + (j * actual_width) - (actual_width * 0.01)\n",
    "            rect_miny = miny + (i * actual_height) - (actual_height * 0.01)\n",
    "            rect_maxx = rect_minx + actual_width + (actual_width * 0.02)\n",
    "            rect_maxy = rect_miny + actual_height + (actual_height * 0.02)\n",
    "                \n",
    "            polygon = Polygon([(rect_minx, rect_miny), (rect_maxx, rect_miny), (rect_maxx, rect_maxy), (rect_minx, rect_maxy)])\n",
    "                \n",
    "            if polygon.intersects(geometry):\n",
    "                rectangles.append(polygon)\n",
    "        \n",
    "    for index, polygon in tqdm(enumerate(rectangles), total=len(rectangles), desc='Processing'):\n",
    "        temp_file = os.path.join(temp_dir, \"{}_temp.gpkg\".format(index))\n",
    "        if os.path.exists(temp_file):\n",
    "            temp_buildings = gpd.read_file(temp_file)\n",
    "            all_buildings.append(temp_buildings)\n",
    "        \n",
    "                \n",
    "        else:\n",
    "            temp_file = os.path.join(temp_dir, \"{}_temp.gpkg\".format(index))\n",
    "                # Check if the bbox intersects with the input geometry\n",
    "            if polygon.intersects(geometry):\n",
    "                    # Fetch buildings from osmnx within this bbox\n",
    "                    #try:\n",
    "                temp_buildings = ox.features_from_polygon(polygon, tags={'building': True})\n",
    "                \n",
    "                \n",
    "               # print(temp_buildings.columns)\n",
    "               # with open('temp.txt', 'w') as file:\n",
    "               #     file.write('\\n'.join(temp_buildings.columns.astype(str).tolist()))\n",
    "               #     temp_buildings.to_csv('output.csv', index=True)\n",
    "                    \n",
    "                temp_buildings = temp_buildings[temp_buildings.geometry.type.isin(['Polygon', 'MultiPolygon'])]\n",
    "                temp_buildings = temp_buildings[~temp_buildings[\"geometry\"].geom_type.isin([\"LineString\", \"Point\"])]\n",
    "                temp_buildings = temp_buildings.droplevel(\"element_type\")\n",
    "                    \n",
    "                temp_buildings=clean_buildings_osm(temp_buildings, type_dict)\n",
    "                \n",
    "                \n",
    "                if not temp_buildings.empty:\n",
    "                    temp_buildings.to_file(temp_file, driver='GPKG')\t\n",
    "                    all_buildings.append(temp_buildings)\n",
    "                    #except Exception as e:\n",
    "                    #print(f\"Failed to retrieve data for polygon ({polygon}): {str(e)}\")\n",
    "\n",
    "        # Buffer the input geometry and filter buildings\n",
    "    buffer_distance = 10  # meters\n",
    "    buffered_geometry = geometry.buffer(buffer_distance)\n",
    "    all_buildings = all_buildings[all_buildings.intersects(buffered_geometry)]\n",
    "\n",
    "        # Clean up the data\n",
    "    all_buildings=pd.concat(all_buildings)\n",
    "    all_buildings = all_buildings.drop_duplicates().reset_index(drop=True)\n",
    "    all_buildings['id'] = all_buildings['id'].apply(_replace_nan_with_uuid)\n",
    "        #shutil.rmtree(temp_dir)\n",
    "        \n",
    "    return all_buildings\n",
    "\n",
    "#data_root = \"data/insar_downloads\"  \n",
    "#json_file_path = 'data/insar_data_download_updated.json'        \n",
    "#results = merge_and_clip_geopackages_for_all_cities(data_root, json_file_path)\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be707141-ed81-4aef-9fd4-53d0b68d3b7e",
   "metadata": {},
   "source": [
    "### Merge ascending and descending tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07522c-3a3d-4a30-9d3e-09caacea2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def merge_and_clip_geopackages_for_all_cities(data_root, json_config_path):\n",
    "    \n",
    "    def _calculate_intersection(ascending, descending):\n",
    "        union_ascending = unary_union([box(*url['bbox']) for url in ascending if 'bbox' in url])\n",
    "        union_descending = unary_union([box(*url['bbox']) for url in descending if 'bbox' in url])\n",
    "        intersection=union_ascending.intersection(union_descending)\n",
    "        return intersection\n",
    "\n",
    "    \n",
    "    def _process_and_store_gdf(data_root, country, city_name, direction, files, intersection_area):\n",
    "        merged_gdf = gpd.GeoDataFrame()\n",
    "        for file in files:\n",
    "            url = file['url']\n",
    "            folder_name = os.path.basename(url).split('.')[0]\n",
    "            folder_path = os.path.join(data_root, country, city_name, direction, folder_name)\n",
    "            file_path_without_extension = os.path.join(folder_path, folder_name)\n",
    "\n",
    "            # Load data using the intersection area as a mask\n",
    "            progress.set_postfix_str(f\"Reading {folder_name} in {city_name}/{direction}\")\n",
    "            gdf = gpd.read_file(f\"{file_path_without_extension}.geojson\", engine=\"pyogrio\", bbox = box(*intersection_area.bounds))\n",
    "             \n",
    "            # Concatenate to the merged GeoDataFrame\n",
    "            merged_gdf = pd.concat([merged_gdf, gdf[gdf.intersects(intersection_area)]], ignore_index=True)\n",
    "            progress.update(1)\n",
    "           \n",
    "        # Save clipped GeoJSON files\n",
    "        clipped_file_path = os.path.join(data_root, country, city_name, f\"{direction.lower()}_clipped.geojson\")\n",
    "        if not merged_gdf.empty:\n",
    "            merged_gdf.to_file(clipped_file_path, driver='GeoJSON')\n",
    "            \n",
    "        # Clear memory\n",
    "        merged_gdf = None\n",
    "\n",
    "        return clipped_file_path\n",
    "    \n",
    "   \n",
    "    # Load the JSON configuration\n",
    "    with open(json_config_path, 'r') as file:\n",
    "        json_config = json.load(file)\n",
    "\n",
    "    # Set up progress tracking\n",
    "    results=[]\n",
    "    #total_iterations = sum(len(urls) for cities in json_config.values() for directions in cities.values() for urls in directions.values())\n",
    "    total_iterations = sum(len(urls) for cities in json_config.values() for directions in cities.values() for key, urls in directions.items() if key != 'Intersection')\n",
    "\n",
    "    progress = tqdm(total=total_iterations, desc='Processing Cities')\n",
    "    \n",
    "    for country, cities in json_config.items():\n",
    "        for city_name, directions in cities.items():\n",
    "            if 'Intersection' in directions and directions['Intersection']:  # Skip if intersection is already calculated\n",
    "                progress.update(len(directions['Ascending']) + len(directions['Descending']))\n",
    "                continue\n",
    "                \n",
    "            # Calculate intersection if not already calculated\n",
    "            if 'Intersection' not in directions or not directions['Intersection']:\n",
    "                intersection_area = _calculate_intersection(directions['Ascending'], directions['Descending'])\n",
    "                directions['Intersection'] = dumps(intersection_area)  # Save as WKT\n",
    "                \n",
    "            else:\n",
    "                intersection_area = loads(directions['Intersection'])\n",
    "\n",
    "            # Process Ascending\n",
    "            ascending_result = _process_and_store_gdf(data_root, country, city_name, 'Ascending', directions['Ascending'], intersection_area)\n",
    "            descending_result = _process_and_store_gdf(data_root, country, city_name, 'Descending', directions['Descending'], intersection_area)\n",
    "            \n",
    "            # Save updated JSON configuration after each city is processed\n",
    "            with open(json_config_path, 'w') as f:\n",
    "                json.dump(json_config, f, indent=4)\n",
    "                \n",
    "            results.append({\"ascending_clipped\": ascending_result,\"descending_clipped\": descending_result,\"intersection_area\": intersection_area})\n",
    "\n",
    "\n",
    "    progress.set_postfix_str(f\"Complete\")\n",
    "    progress.close()\n",
    "    return results\n",
    "\n",
    "#TODO: incluse origing as filename and simplify generation by skipping clipping here move calculate intersection to next block"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
