"""
This code is generated by Ridvan Salih KUZU
LAST EDITED:  20.02.2023
ABOUT SCRIPT:
It defines training and evaluation functions
"""
import sys
sys.path.append("../..")

#import sys; sys.argv=['']; del sys
from tqdm import tqdm
import optuna
from optuna.samplers import TPESampler
import argparse
import warnings
import joblib
import gc
import numpy as np
from represent.tools.utils_uc3 import  AvgMeter, get_lr, anomaly_score_reports,frame_anomaly_table
from represent.datamodules.uc3_datamodule import Dataloader
import os
import copy
from represent.losses.soft_dtw import SoftDTWLoss,CustomL1Loss
from represent.models.uc3_lstm_autoencoder import RecurrentAutoencoder
from represent.models.uc3_benchmark.dense_autoencoder import DenseAutoencoder

import torch

warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def train_validate(model, data_loader,learning_rate, penalty_rate,loss_type, output_file_template, device, step_save=False,gamma=0.1):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: LSTM model to be trained or fine-tuned
        :param data_loader: data loader
        :param learning_rate: learning rate for optimization
        :param penalty_rate: penalty rate for optimization
        :param loss_type: l1 or dtw to select the loss function
        :param output_file_template: file name template to save the fine-tuned uc1_resnet_base.py and inference statistics
        :param device: GPU or CPU device
        :return:
    '''

    best_model=None
    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), betas=(0.9,0.98),eps=1e-6, lr=learning_rate,weight_decay=penalty_rate)  # weight_decay=0.0001
    #optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)  # model.parameters is the parameters that optimizerd have to change

    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    if loss_type=="l1":
        loss_criterion = CustomL1Loss().to(device)
    else:
        loss_criterion=SoftDTWLoss(gamma=gamma).to(device)

    step = "epoch"
    best_loss = float('inf')
    for epoch in range(args.total_epoch):
        print(f"Epoch: {epoch + 1}")
        model.train()
        train_epoch(model, data_loader.get_data_loader('train'), optimizer, lr_scheduler, loss_criterion, step, device)
        model.eval()
        with torch.no_grad():
            valid_loss, image_batch = valid_epoch(model, data_loader.get_data_loader('valid'), loss_criterion, device)
            if step_save: test_evaluate(model, data_loader, loss_type, output_file_template, device,epoch)


        if valid_loss.avg < best_loss:
            best_loss = valid_loss.avg
            best_model = copy.deepcopy(model)
            #torch.jit.save(torch.jit.trace(model, (image_batch)),'{}_model_best.pth.tar'.format(output_file_template))
            torch.save(model.state_dict(), '{}_model_best.pth.tar'.format(output_file_template))
            print("INFO: Best Model saved into " + '{}_model_best.pth.tar'.format(output_file_template))

        lr_scheduler.step(valid_loss.avg)
    return best_model

def train_epoch(model, train_loader, optimizer, lr_scheduler, loss_criterion, step, device):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: LSTM model to be trained or fine-tuned
         :param train_loader: data loader for training files
         :param optimizer: model optimizer object
         :param lr_scheduler: scheduler object to update the learning rate for optimization
         :param loss_criterion: loss objective function to be utilized for backprop
         :param step: epoch step number
         :param device: GPU or CPU device
         :return: returns average training loss
    '''

    loss_meter = AvgMeter()
    tqdm_object = tqdm(train_loader, total=len(train_loader))
    for data_batch, permuted_data_batch, unique_id_batch, foot_batch, _ in tqdm_object:
        output = model(permuted_data_batch.to(device))
        loss = loss_criterion(output,data_batch.to(device))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if step == "batch":
            lr_scheduler.step()

        count = data_batch.size(1)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))
    return loss_meter


def valid_epoch(model, valid_loader, loss_criterion, device):
    '''
         THIS FUNCTION RUNS THE SINGLE TRAINING EPOCHS
         :param model: LSTM model to be trained or fine-tuned
         :param valid_loader: data loader for validation files
         :param loss_criterion: loss objective function to be utilized for backprop
         :param device: GPU or CPU device
         :return: returns the tuple of ( average validation loss, an image batch example, a text batch example).
                  image and text batches are used as template to save the model if it reaches the optimum
    '''

    loss_meter = AvgMeter()

    tqdm_object = tqdm(valid_loader, total=len(valid_loader))
    for data_batch, permuted_data_batch, unique_id_batch, foot_batch, _ in tqdm_object:
        output = model(permuted_data_batch.to(device))
        loss = loss_criterion(output,data_batch.to(device))

        count = data_batch.size(1)
        loss_meter.update(loss.item(), count)

        tqdm_object.set_postfix(valid_loss=loss_meter.avg)
    return loss_meter, data_batch.to(device)


def test_evaluate(model, data_loader, loss_type, output_file_template, device,step="final",gamma=0.1):
    '''
           THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
           :param model: LSTM model to be evaluated or tested
           :param data_loader: data loader
           :param loss_type: learning rate for optimization
           :param penalty_rate: penalty rate for optimization
           :param loss_type: l1 or dtw to select the loss function
           :param output_file_template: file name template to save the fine-tuned uc1_resnet_base.py and inference statistics
           :param device: GPU or CPU device
           :return:
       '''

    if loss_type == "l1":
        loss_criterion = CustomL1Loss(reduction='none').to(device)
    else:
        loss_criterion=SoftDTWLoss(reduction='none',gamma=gamma).to(device)

    model.eval()
    df_test=eval_epoch(model, data_loader.get_data_loader('test'), loss_criterion, device)
    df_eval = eval_epoch(model, data_loader.get_data_loader('eval'), loss_criterion, device)

    df_report, u, acc = anomaly_score_reports(df_test, df_eval, min_percentile=80, step_resolution=1,per_building=False)

    df_test.to_csv("{}_prediction_results_organic_{}.csv".format(output_file_template,step),index=False)
    df_eval.to_csv("{}_prediction_results_synthetic_{}.csv".format(output_file_template,step),index=False)
    df_report.to_csv("{}_acc_{}_u_{}_prediction_report_{}.csv".format(output_file_template,acc,u,step), index=False)

    return acc


def eval_epoch(model, data_loader, loss_criterion, device):
    '''
        THIS FUNCTION MANAGES THE EXECUTION OF TRAINING AND VALIDATION EPOCHS
        :param model: CLIP model to be fine-tuned
        :param data_loader: data loader for evaluation file
        :param loss_criterion: loss objective function to be utilized for evaluation
        :param device: GPU or CPU device
        :return:
    '''

    loss_list=[]
    unique_list = []
    footprint_list = []
    anomaly_list = []
    with torch.no_grad():
        tqdm_object = tqdm(data_loader, total=len(data_loader))
        for data_batch, permuted_data_batch, unique_id_batch, foot_batch, anomaly_batch in tqdm_object:
            output = model(permuted_data_batch.to(device))
            loss = loss_criterion(output, data_batch.to(device))
            loss_list.append(loss.detach().cpu().numpy())

            unique_list.append(unique_id_batch.detach().cpu().numpy())
            footprint_list.append(foot_batch.detach().cpu().numpy())
            anomaly_list.append(anomaly_batch.detach().cpu().numpy())

    df=frame_anomaly_table(np.concatenate(loss_list,0), np.concatenate(unique_list), np.concatenate(footprint_list),  np.concatenate(anomaly_list))

    return df



def main(args):

    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir,exist_ok=True)

    MANUAL_SEED=42
    LABEL_NAMES = ["trend", "noise", "step"]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(device)
    torch.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with CPU
    torch.cuda.manual_seed(MANUAL_SEED)  # set seed (manually) for generate random numbers with GPU --> "CUDA = GPU"
    np.random.seed(MANUAL_SEED)

    def objective(trial):
        '''
            THIS FUNCTION DEFINES THE OBJECTIVE FUNCTION FOR BAYESIAN HYPERPARAMETER TUNING
            :param trial: trial object of bayesian optimization
            :return: returns weighted F1 score to be maximized
        '''

        gc.collect()

        learning_rate = trial.suggest_categorical('learning_rate', args.learning_rate)
        smooth_window = trial.suggest_categorical('smooth_window', args.smooth_window)
        penalty_rate = trial.suggest_categorical('penalty_rate', args.penalty_rate)
        embedding_dim = trial.suggest_categorical('embedding_dim', args.embedding_dim)
        gamma = trial.suggest_categorical('gamma', args.gamma)
        method = trial.suggest_categorical('method', args.method)

        is_normalize = args.is_normalize #trial.suggest_categorical('is_normalize', [True])
        is_detrend = args.is_detrend#trial.suggest_categorical('is_detrend', [False])
        loss_type = args.loss_type
        lstm_stack=args.lstm_stack
        is_permute=args.is_permute
        is_debug = args.is_debug
        is_gradient=args.is_gradient

        print(f"INFO: Trial number: {trial.number}")
        print(f"INFO: Learning rate: {learning_rate}")
        print(f"INFO: Penalty rate: {penalty_rate}")
        print(f"INFO: Embedding dimension: {embedding_dim}")
        print(f"INFO: Smooth window: {smooth_window}")
        print(f"INFO: Is normalize: {is_normalize}")
        print(f"INFO: Is detrend: {is_detrend}")
        print(f"INFO: Autoencoder: {method}")

        print(f"INFO: Loss type: {loss_type}")
        print(f"INFO: Gamma: {gamma}")
        print(f"INFO: LSTM stack: {lstm_stack}")
        print(f"INFO: Data Permutation: {is_permute}")
        print(f"INFO: Data Gradient: {is_gradient}")
        print(f"INFO: DEBUG mode: {is_debug}\n")

        data_loader = Dataloader(args.train_dir,
                                 args.test_dir,
                                 args.eval_dir,
                                 split=0.25,
                                 batch_size=args.batch_size,
                                 window_length=smooth_window,
                                 num_workers=args.num_workers,
                                 is_normalize=is_normalize,
                                 is_detrend=is_detrend,
                                 is_permute=is_permute,
                                 is_gradient=is_gradient,
                                 is_debug=is_debug,
                                 seed=MANUAL_SEED)


        output_file_template = '{}ls_{}_lt_{}_lr_{}_pr_{}_sw_{}_in_{}_id_{}_ip_{}_ig_{}_me_{}_ed_{}_gm_{}_tn_{}'.format(args.log_dir,lstm_stack,loss_type,learning_rate,penalty_rate,smooth_window,is_normalize,is_detrend,is_permute,is_gradient,method,embedding_dim,gamma, trial.number)
        if method=="lae":
            model = RecurrentAutoencoder(data_loader.get_data_dimensions()[0], data_loader.get_data_dimensions()[-1], embedding_dim)
        elif method == "ae":
            model = DenseAutoencoder(data_loader.get_data_dimensions()[0], data_loader.get_data_dimensions()[-1],embedding_dim)


        model = model.to(device)

        best_model = train_validate(model, data_loader,learning_rate, penalty_rate,loss_type, output_file_template, device,args.step_save,gamma)
        acc=test_evaluate(best_model, data_loader,loss_type, output_file_template, device,gamma=gamma)
        print(f"INFO: Accuracy @ 80 percentile: {acc}\n")
        return acc

    study = optuna.create_study(sampler=TPESampler(), direction='maximize',
                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5,
                                                                   n_warmup_steps=24,
                                                                   interval_steps=6))
    log_file = args.log_dir + 'optimization_logs.pkl'
    #if os.path.isfile(log_file):
    #    study = joblib.load(log_file)

    study.optimize(objective, n_trials=args.n_trials, gc_after_trial=True)
    joblib.dump(study, log_file)


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--train-dir', type=str, default='../data/database_train.csv')
    parser.add_argument('--test-dir', type=str, default='../data/database_test.csv')
    parser.add_argument('--eval-dir', type=str, default='../data/valdataset_3_3_3.csv')
    parser.add_argument('-b', '--batch-size', default=128, type=int, metavar='BS',help='number of batch size (default: 32)')
    parser.add_argument('--log-dir', type=str, default='UC3/results/optuna/')
    parser.add_argument('--total-epoch', type=int, default=1)
    parser.add_argument('--num-workers', type=int, default=8)
    parser.add_argument('--embedding-dim', type=int, nargs='+', default=[32])
    parser.add_argument('--lstm-stack', type=int, default=1)
    parser.add_argument('--loss-type', type=str, default='l1')
    parser.add_argument('--gamma', type=float, nargs='+', default=[0.1, 0.01, 0.05, 0.5])
    parser.add_argument('-p', '--is-permute', dest='is_permute', action='store_true')
    parser.add_argument('-d', '--is-debug', dest='is_debug', action='store_true')
    parser.add_argument('-s', '--step-save', dest='step_save', action='store_true')
    parser.add_argument('-n', '--is-normalize', dest='is_normalize', action='store_true')
    parser.add_argument('-t', '--is-detrend', dest='is_detrend', action='store_true')
    parser.add_argument('-g', '--is-gradient', dest='is_gradient', action='store_true')

    parser.add_argument('--n-trials', type=int, default=1)

    parser.add_argument('--method', type=str, nargs='+', default=["lae","ae"])  #

    parser.add_argument('--smooth-window', type=int, nargs='+', default=[1, 3, 5, 7, 9, 11,13])#
    parser.add_argument('--learning-rate', type=float, nargs='+', default=[1e-3, 5e-4, 1e-4, 5e-5])
    parser.add_argument('--penalty-rate', type=float, nargs='+', default=[0,1e-2,5e-3, 1e-3, 5e-4]) #1e-2,5e-3, 1e-3, 5e-4,

    args = parser.parse_args()

    main(args)
 

